llm:
  # default provider used when UI/API does not specify a model
  default: solar

  providers:
    openai:
      model: gpt-4o-mini

    gemini:
      model: gemini-2.5-flash
      #model: gemini-1.5-flash-latest  # ✅ 또는
      #model: gemini-1.5-flash          # ✅ (현재 설정)

    claude:
      model: claude-3-5-sonnet-20241022  # ✅ 최신
      # 또는
      model: claude-3-sonnet-20240229    # ✅ 구버전

    # Upstage SOLAR (OpenAI-compatible endpoint)
    solar:
      model: solar-1-mini-chat  # ✅ 가장 일반적
      # 또는
      model: solar-pro             # ✅ Pro 버전

    # Local CPU LLM (llama.cpp server container)
    local:
      endpoint: http://local-llm:8080
      model: tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf  # ✅ 변경 후

gateway:
  timeout_seconds: 60
  cors_allow_origins:
    - http://localhost:3000
    - http://127.0.0.1:3000

monitoring:
  traces_keep_last: 50

pricing_per_1k_tokens_usd:
  # NOTE: placeholder rates for learning purposes only.
  openai: 0.001
  gemini: 0.001
  claude: 0.003
  solar: 0.0005
  local: 0.0
